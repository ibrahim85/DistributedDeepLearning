{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Data Machine Learning, Distributed Machine Learning, Parallel Machine Learning with C++, CUDA, Scala, Spark\n",
    "=============\n",
    "\n",
    "Ron Wu\n",
    "-------------\n",
    "\n",
    "10/21/16\n",
    "\n",
    "Reference: free courses from the creators of\n",
    "<br>NVIDIA https://developer.nvidia.com/udacity-cs344-intro-parallel-programming<br>\n",
    "Scala http://www.scala-lang.org/blog/2016/05/23/scala-moocs-specialization-launched.html<br>\n",
    "Spark, Databricks https://databricks.com/blog/2016/06/01/databricks-to-launch-first-of-five-free-big-data-courses-on-apache-spark.html <br> \n",
    "### Contents\n",
    "\n",
    "\n",
    "1. <a href =#scala>Functional and Object-Oriented Scala</a>\n",
    "     - <a href=#scalab>Functional Aspect of Scala</a>\n",
    "     - <a href = #relu>Object-Oriented Aspect of Scala: Data-Parallelism</a> \n",
    "     - <a href = #relu>Scala with Spark</a> \n",
    "<br><br>\n",
    "2. <a href=#Spark>Big Data, Distributed Analysis with Spark</a>\n",
    "     - <a href =#entropy>Spark with Time Series: spark-ts</a> \n",
    "     - <a href =#entropy>Spark with TensorFlow: TensorFrames</a> \n",
    "     - <a href =#entropy>Other Spark Ecosystem</a> \n",
    "<br><br>\n",
    "3. <a href =#convnet>Parallel with CUDA, C++</a>\n",
    "     - <a href=#worked>Parallel Computing</a> \n",
    "     - <a href=#worked>GPU Programming</a> \n",
    "<br><br>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = 'scala'></a>\n",
    "# Functional and Object-Oriented Scala\n",
    "\n",
    "reference:\n",
    "http://docs.scala-lang.org/overviews/\n",
    "\n",
    "<a name = 'scalab'></a>\n",
    "## Functional Aspect of Scala\n",
    "\n",
    "For easier transition to parallel programming, throughout this notebook I will try to use functional style over imperative style whenever possible. That is because in parallel programming variables are immutable. Once it is set, it cannot be changed, which avoids a lot deadlocks. \n",
    "\n",
    "If a block of code is written in loop fashion, the compiler will turn it into functions, the iteration and any change of variables inside the loop are passing through the function calls. Thus whether you write it in serial or recursion, it will end up in recursive call anyway.  \n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.scala\n"
     ]
    }
   ],
   "source": [
    "%%file main.scala\n",
    "\n",
    "object main extends App {\n",
    "  //object means static in Java, hence singleton\n",
    "    \n",
    "  var str = \"The answer is \"\n",
    "\n",
    "  def fib(n: Int) = {\n",
    "\n",
    "    // recursive function has to have explicit return type\n",
    "    // : is optional unless {} is not used or not on the same line\n",
    "    // fibIter is hepler, is only accessible inside of fib. This helps namespace pollution\n",
    "    // inside function can see variables outside such as str, but outside cannot see inside\n",
    "\n",
    "    def fibIter(i: Int, a: Int, b: Int): String = \n",
    "     if (i == n) str + a else fibIter(i+1, b, a+b) \n",
    "    // above is tail-recursive, hence will not build up the stack\n",
    "    \n",
    "    //like in python the last line is return\n",
    "    fibIter(0, 0, 1)\n",
    "  }\n",
    "  println(fib(10))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is 55\r\n"
     ]
    }
   ],
   "source": [
    "! scalac main.scala\n",
    "! scala main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we write above fibonacci even shorter?\n",
    "\n",
    "Yes, use high-order function, i.e. pass in anonymous function to function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='Spark'></a>\n",
    "# Big Data with Spark\n",
    "\n",
    "- Apache Spark Components \n",
    "    - Spark SQL\n",
    "    - Spark Streaming\n",
    "    - MLlib\n",
    "    - GraphX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
